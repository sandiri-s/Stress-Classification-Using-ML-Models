{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as scy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from sklearn.metrics import accuracy_score\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import tables as tb\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "def readFile(filename):\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        data = pd.read_csv(csvfile)\n",
    "    #print(data)\n",
    "    colsName = data.columns.values\n",
    "    #print(colsName)\n",
    "    featuresName = colsName[:len(colsName)-1]\n",
    "    labelName = colsName[len(colsName)-1:len(colsName)]\n",
    "    print(featuresName)\n",
    "\n",
    "    x_data = data[featuresName].values\n",
    "    y_data = data[labelName].values\n",
    "    X_data = preprocessdata(x_data) #normalize\n",
    "    #print (x_data)\n",
    "    #print (y_data)\n",
    "    return (X_data, y_data)\n",
    "\n",
    "# Standardize the data since attributes are of varying scales\n",
    "def preprocessdata(X):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    res_X = scaler.fit_transform(X)\n",
    "    return(res_X)\n",
    "\n",
    "def splitData(X, y, tstsize):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = tstsize)\n",
    "    return(X_train, X_test, y_train, y_test)\n",
    "\n",
    "def SelectFeactureUnivar(X,y):\n",
    "    yy = y.flatten()\n",
    "    selector = SelectKBest(f_classif, k = 3)\n",
    "    selector.fit(X,yy)\n",
    "    FeatureNames = []\n",
    "    print(selector.pvalues_)\n",
    "    X_new = selector.fit_transform(X, yy)\n",
    "    #print(X_new.shape)\n",
    "    return X_new\n",
    "            \n",
    "def SelectFeacturePCA(X,y):\n",
    "    scaler = StandardScaler()\n",
    "    res_X = scaler.fit_transform(X)\n",
    "    X_norm, y_norm =res_X , y.flatten()\n",
    "    #print(X_norm)\n",
    "    pca = PCA(n_components=3) \n",
    "    fitval = pca.fit(X_norm)\n",
    "    X_pca = pca.fit_transform(X_norm)\n",
    "    varience = pca.explained_variance_\n",
    "    print(varience)\n",
    "    return X_pca\n",
    "    \n",
    "\n",
    "X, y = readFile('Dataset\\All_subjects.csv')\n",
    "X_new = SelectFeactureUnivar(X,y)\n",
    "X_pca = SelectFeacturePCA(X,y)\n",
    "OX_trn, OX_tst, Oy_trn, Oy_tst = splitData(X, y, tstsize = 0.2)\n",
    "X_trn, X_tst, y_trn, y_tst = splitData(X_new, y, tstsize = 0.2)\n",
    "PX_trn, PX_tst, Py_trn, Py_tst = splitData(X_pca, y, tstsize = 0.2)\n",
    "#X_new2 = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##\"Next Cross Validate --split\"\n",
    "#\"Normalize data using\"\n",
    "# Visualize training using PCA\n",
    "# Feature selection http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Dataset Visualization using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "res_X = scaler.fit_transform(OX_trn)\n",
    "X_norm, y_norm =res_X ,Oy_trn.flatten()\n",
    "print(X_norm.shape)\n",
    "pca = PCA(n_components=2) \n",
    "fitval = pca.fit(X_norm)\n",
    "pca_X = pd.DataFrame(pca.fit_transform(X_norm))\n",
    "varience = pca.explained_variance_\n",
    "print(\"Original shape:   \", X_trn.shape)\n",
    "print(\"Transformed shape:\", pca_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_X[y_norm == 1][0], pca_X[y_norm == 1][1], s = 60, label='relax', c='navy')\n",
    "plt.scatter(pca_X[y_norm == 2][0], pca_X[y_norm == 2][1], s = 60, label='physical', c='turquoise')\n",
    "plt.scatter(pca_X[y_norm == 5][0], pca_X[y_norm == 5][1], s = 60, label='cognitive', c='darkorange')\n",
    "plt.scatter(pca_X[y_norm == 7][0], pca_X[y_norm == 7][1], s = 60, label='emotional', c='red')\n",
    "\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with simplified SMO algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SVMSMO_Model():\n",
    "    def __init__(self, max_iter=10000, k='linear', C=1.0, tol=0.001):\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.tol = tol \n",
    "        #self.kernel_\n",
    "        if k == 'linear':\n",
    "            self.kernel = self.kernel_linear\n",
    "\n",
    "    def get_rnd_int(self, j,k,l):\n",
    "        i = l\n",
    "        c=0\n",
    "        while i == l and c<1000:\n",
    "            i = rnd.randint(j,k)\n",
    "            c+=1\n",
    "        return i\n",
    "    \n",
    "    def kernel_linear(self, x1, x2):\n",
    "        return np.dot(x1, x2.T)\n",
    "    \n",
    "    def bias(self, X, y, w):\n",
    "        b = np.mean(y - np.dot(w.T, X.T))\n",
    "        return b\n",
    "    \n",
    "    def weights(self, alpha, y, X):\n",
    "        w = np.dot(alpha * y, X)\n",
    "        return w\n",
    "    \n",
    "    def fx_i(self, X, w, b):\n",
    "        prb = np.dot(w.T, X.T) + b\n",
    "        fx = np.sign(np.dot(w.T, X.T) + b).astype(int)\n",
    "        return fx, prb\n",
    "    \n",
    "    def comLandH(self, C, oldalpha_i, oldalpha_j, yi, yj):\n",
    "        if(yj != yi):\n",
    "            return (max(0,  oldalpha_i -  oldalpha_j), min(C, C + oldalpha_i - oldalpha_i))\n",
    "        else:\n",
    "            return (max(0,  oldalpha_j +  oldalpha_i - C), min(C,  oldalpha_j +  oldalpha_i))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        alphas = np.zeros((m))\n",
    "        kernel = self.kernel\n",
    "        passes = 0\n",
    "        while True:\n",
    "            passes += 1\n",
    "            old_alphas = np.copy(alphas)\n",
    "            for i in range(0, m):\n",
    "                j = self.get_rnd_int(0, m-1, i) # Get random j\n",
    "                #x_i, x_j, y_i, y_j = X[i,:], X[j,:], y[i], y[j]\n",
    "                eta = kernel(X[j,:], X[j,:]) + kernel(X[i,:], X[i,:]) - 2 * kernel(X[j,:], X[i,:])\n",
    "                \n",
    "                if eta == 0:\n",
    "                    continue\n",
    "                    \n",
    "                oldalpha_i, oldalpha_j = alphas[i], alphas[j]\n",
    "                (L, H) = self.comLandH(self.C, oldalpha_i, oldalpha_j, y[i], y[j])\n",
    "\n",
    "                # parameters\n",
    "                self.w = self.weights(alphas, y, X)\n",
    "                self.b  = self.bias(X, y, self.w)\n",
    "\n",
    "                # Get E_i, E_j\n",
    "                E_i = self.fx_i(X[i,:], self.w, self.b)[0] - y[i]\n",
    "                E_j = self.fx_i(X[j,:], self.w, self.b)[0] - y[j]\n",
    "                \n",
    "                # update alphas\n",
    "                alphas[i] = oldalpha_i + np.float(y[i] * (E_j - E_i))/eta\n",
    "                alphas[i] = max(alphas[i], L)\n",
    "                alphas[i] = min(alphas[i], H)\n",
    "\n",
    "                alphas[j] =  oldalpha_j + y[i]*y[j] * ( oldalpha_i - alphas[i])\n",
    "\n",
    "            #Are the alphas converging\n",
    "            change = np.linalg.norm(alphas - old_alphas)\n",
    "            if change < self.tol:\n",
    "                break\n",
    "\n",
    "            if passes >= self.max_iter:\n",
    "                print(\"Reached a max of %d iterations\" % (self.max_iter))\n",
    "                return\n",
    "            \n",
    "        \n",
    "        self.b = self.bias(X, y, self.w)\n",
    "        self.w = self.weights(alphas, y, X)\n",
    "        #print(self.w)\n",
    "        \n",
    "        #support vectors can be computed as follows\n",
    "        alp_idx = np.where(alphas > 0)[0]\n",
    "        support_vectors = X[alp_idx, :]\n",
    "        \n",
    "        return self.w, self.b\n",
    "    \n",
    "    def predict(self, X_tst):\n",
    "        y, prb = self.fx_i(X_tst, self.w, self.b)\n",
    "        return y, prb \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train four svm models \n",
    "(One vs all wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TrainModel(X_trn,y_trn):\n",
    "    SVMs = []\n",
    "    classes = [1,2,5,7]\n",
    "    fits = []\n",
    "    for i in range(len(classes)):\n",
    "        #print ('\\nThe %d/%dth classifier training...' % (i+1, len(classes)))\n",
    "        y_train = deepcopy(y_trn)\n",
    "        idx_i = y_train == i\n",
    "        y_train[idx_i] = 1\n",
    "        y_train[~idx_i] = -1\n",
    "        model = SVMSMO_Model(max_iter=10000, k ='linear', C=1.0, tol=0.001)\n",
    "        fit = model.fit(X_trn, y_train)\n",
    "        fits.append(fit)\n",
    "        SVMs.append(model)\n",
    "    return SVMs, fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def onevsall_pred(SVMs, X, classes):\n",
    "    NumClasses = len(classes)\n",
    "    print(NumClasses)\n",
    "    predscore = np.zeros((NumClasses, X.shape[0]))\n",
    "    #print(predscore)\n",
    "    for i in range (NumClasses):\n",
    "        svm = SVMs[i]\n",
    "        predscore[i, :] = svm.predict(X)[1]\n",
    "    pred = np.argmax(predscore, axis=0)\n",
    "    y_pred = deepcopy(pred) \n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 0 :\n",
    "            y_pred[i] = 1 \n",
    "        elif y_pred[i] == 1:\n",
    "            y_pred[i] = 2 \n",
    "        elif y_pred[i] == 2:\n",
    "            y_pred[i] = 5\n",
    "        elif y_pred[i] == 1:\n",
    "            y_pred[i] = 7 \n",
    "    \n",
    "\n",
    "    #print(y_pred)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y, y_pred):\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    return acc\n",
    "#function as illustrated in sklearn \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Evaluated  using 70-30 Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Oy_train = Oy_trn.flatten()\n",
    "Oy_test = Oy_tst.flatten()\n",
    "SVMs, losses = TrainModel(OX_trn, Oy_train)\n",
    "Opred_ytrn = onevsall_pred(SVMs, OX_trn, classes= [1,2,5,7])\n",
    "Oy_pred = onevsall_pred(SVMs, OX_tst, classes= [1,2,5,7])\n",
    "print('Training  accuracy: %f' % (accuracy(Oy_train, Opred_ytrn)))\n",
    "print ('Test accuracy: %f' % (accuracy(Oy_test, Oy_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cnf_matrix = confusion_matrix(Oy_test, Oy_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "class_names = ['relaxed', 'physical', 'cognitive',  'emotional']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with 3 Best features from Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = y_trn.flatten()\n",
    "y_test = y_tst.flatten()\n",
    "SVMs, losses = TrainModel(X_trn, y_train)\n",
    "pred_ytrn = onevsall_pred(SVMs, X_trn, classes= [1,2,5,7])\n",
    "#print(pred_train_one_vs_all)\n",
    "y_pred = onevsall_pred(SVMs, X_tst, classes= [1,2,5,7])\n",
    "print('Training dataset accuracy: %f' % (accuracy(y_train, pred_ytrn)))\n",
    "print ('Test datast accuracy: %f' % (accuracy(y_tst, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "class_names = ['relaxed', 'physical', 'cognitive',  'emotional']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with 3 features from PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Py_train = Py_trn.flatten()\n",
    "Py_test = Py_tst.flatten()\n",
    "\n",
    "SVMs, losses = TrainModel(PX_trn, Py_train)\n",
    "Ppred_ytrn = onevsall_pred(SVMs, PX_trn, classes= [1,2,5,7])\n",
    "Py_pred = onevsall_pred(SVMs, PX_tst, classes= [1,2,5,7])\n",
    "print('Training dataset accuracy: %f' % (accuracy(Py_train, pred_ytrn)))\n",
    "print ('Test datast accuracy: %f' % (accuracy(Py_test, Py_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(Py_test, Py_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "class_names = ['relaxed', 'physical', 'cognitive',  'emotional']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models evaluated using Leave one Subject out Cross validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupsO = np.array([1, 1, 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 , 3 , 3 , 4 , 4 , 4 , 4 ,\n",
    "     5 , 5 , 5 , 5 ,  6 ,  6 ,  6 ,  6 ,  7 ,  7 ,  7 ,  7 ,  8 ,  8 ,  8 ,  8 ,\n",
    "      9 ,  9 ,  9 ,  9 ,  10 ,  10 ,  10 ,  10 ,  11 ,  11 ,  11 ,  11 ,  12 ,  12 ,  12 ,  12 ,\n",
    "      13 ,  13 ,  13 ,  13 ,  14 ,  14 ,  14 ,  14 ,  15 ,  15 ,  15 ,  15 ,  16 ,  16 ,  16 ,  16])\n",
    "\n",
    "loso = LeaveOneGroupOut()\n",
    "OTrain_acc = []\n",
    "OTest_acc =[]\n",
    "for train_index, test_index in loso.split(X, y, groupsO):\n",
    "    OX_train, OX_test = X[train_index], X[test_index]\n",
    "    Oy_train, Oy_test = y[train_index], y[test_index]\n",
    "    Oy_train = Oy_train.flatten()\n",
    "    Oy_test = Oy_test.flatten()\n",
    "    OSVMs, Olosses = TrainModel(OX_train, Oy_train)\n",
    "    Opred_ytrn = onevsall_pred(OSVMs, OX_train, classes= [1,2,5,7])\n",
    "    Oy_pred = onevsall_pred(OSVMs, OX_test, classes= [1,2,5,7])\n",
    "    Otrnacc = accuracy(Oy_train, Opred_ytrn)\n",
    "    Otstacc = accuracy(Oy_test, Oy_pred)\n",
    "    OTrain_acc.append(Otrnacc)\n",
    "    OTest_acc.append(Otstacc)\n",
    "\n",
    "Otrain = np.array(OTrain_acc)\n",
    "OTrnAcc=np.mean(Otrain)\n",
    "\n",
    "Otest = np.array(OTest_acc)\n",
    "OTstAcc=np.mean(Otest)\n",
    "\n",
    "print('Training dataset accuracy: %f' % (OTrnAcc))\n",
    "print ('Test datast accuracy: %f' % (OTstAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Model with 3 Best features from Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = np.array([1, 1, 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 , 3 , 3 , 4 , 4 , 4 , 4 ,\n",
    "     5 , 5 , 5 , 5 ,  6 ,  6 ,  6 ,  6 ,  7 ,  7 ,  7 ,  7 ,  8 ,  8 ,  8 ,  8 ,\n",
    "      9 ,  9 ,  9 ,  9 ,  10 ,  10 ,  10 ,  10 ,  11 ,  11 ,  11 ,  11 ,  12 ,  12 ,  12 ,  12 ,\n",
    "      13 ,  13 ,  13 ,  13 ,  14 ,  14 ,  14 ,  14 ,  15 ,  15 ,  15 ,  15 ,  16 ,  16 ,  16 ,  16])\n",
    "\n",
    "loso = LeaveOneGroupOut()\n",
    "Train_acc = []\n",
    "Test_acc =[]\n",
    "for train_index, test_index in loso.split(X_new, y, groups):\n",
    "    X_trn, X_tst = X[train_index], X[test_index]\n",
    "    y_trn, y_test = y[train_index], y[test_index]\n",
    "    y_train = y_trn.flatten()\n",
    "    y_test = y_tst.flatten()\n",
    "    SVMs, Olosses = TrainModel(X_trn, y_train)\n",
    "    pred_ytrn = onevsall_pred(SVMs, X_trn, classes= [1,2,5,7])\n",
    "    y_pred = onevsall_pred(SVMs, X_tst, classes= [1,2,5,7])\n",
    "    trnacc = accuracy(y_train, pred_ytrn)\n",
    "    tstacc = accuracy(y_test, y_pred)\n",
    "    Train_acc.append(trnacc)\n",
    "    Test_acc.append(tstacc)\n",
    "\n",
    "train = np.array(Train_acc)\n",
    "TrnAcc=np.mean(train)\n",
    "\n",
    "test = np.array(Test_acc)\n",
    "TstAcc=np.mean(test)\n",
    "\n",
    "print('Training dataset accuracy: %f' % (OTrnAcc))\n",
    "print ('Test datast accuracy: %f' % (OTstAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Model with 3 features from PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupsP = np.array([1, 1, 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 , 3 , 3 , 4 , 4 , 4 , 4 ,\n",
    "     5 , 5 , 5 , 5 ,  6 ,  6 ,  6 ,  6 ,  7 ,  7 ,  7 ,  7 ,  8 ,  8 ,  8 ,  8 ,\n",
    "      9 ,  9 ,  9 ,  9 ,  10 ,  10 ,  10 ,  10 ,  11 ,  11 ,  11 ,  11 ,  12 ,  12 ,  12 ,  12 ,\n",
    "      13 ,  13 ,  13 ,  13 ,  14 ,  14 ,  14 ,  14 ,  15 ,  15 ,  15 ,  15 ,  16 ,  16 ,  16 ,  16])\n",
    "\n",
    "loso = LeaveOneGroupOut()\n",
    "PTrain_acc = []\n",
    "PTest_acc =[]\n",
    "for train_index, test_index in loso.split(X_pca, y, groupsP):\n",
    "    PX_trn, PX_tst = X[train_index], X[test_index]\n",
    "    Py_trn, Py_tet = y[train_index], y[test_index]\n",
    "    Py_train = Py_trn.flatten()\n",
    "    Py_test = Py_tst.flatten()\n",
    "    PSVMs, Plosses = TrainModel(PX_trn, Py_train)\n",
    "    Ppred_ytrn = onevsall_pred(PSVMs, PX_trn, classes= [1,2,5,7])\n",
    "    Py_pred = onevsall_pred(SVMs, PX_tst, classes= [1,2,5,7])\n",
    "    Ptrnacc = accuracy(Py_train, Ppred_ytrn)\n",
    "    Ptstacc = accuracy(Py_test, Py_pred)\n",
    "    PTrain_acc.append(Ptrnacc)\n",
    "    PTest_acc.append(Ptstacc)\n",
    "\n",
    "Ptrain = np.array(PTrain_acc)\n",
    "PTrnAcc=np.mean(Ptrain)\n",
    "\n",
    "Ptest = np.array(PTest_acc)\n",
    "PTstAcc=np.mean(Ptest)\n",
    "\n",
    "print('Training dataset accuracy: %f' % (PTrnAcc))\n",
    "print ('Test datast accuracy: %f' % (PTstAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM from a package\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Datasets\n",
    "OXlib= PX_trn\n",
    "OXlibtst= PX_tst\n",
    "\n",
    "#tranforming input binary for fast model training\n",
    "#X_trn = X_train.where((X_train > 0), 1)\n",
    "#print (X_trn)\n",
    "\n",
    "#X = TrainData.ix[:,1:785].values\n",
    "\n",
    "#Y_new = np.array(Y_trn.value)\n",
    "Oylib = Py_trn.flatten()\n",
    "Oylibtst = Py_tst.flatten()\n",
    "\n",
    "\n",
    "#print(X.shape)\n",
    "#print(y.shape)\n",
    "#X_test = \n",
    "\n",
    "C = 0.1 # regularization parameter\n",
    "t = 0.001 # tolerance\n",
    "maxiter = 10000 #maximum iteration\n",
    "\n",
    "\n",
    "#Rbf svm\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C, tol=t, max_iter=maxiter).fit(OXlib, Oylib)\n",
    "\n",
    "#Polynomial svm\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C, max_iter=maxiter).fit(OXlib, Oylib)\n",
    "\n",
    "#sigmoid svm\n",
    "sig_svc = svm.SVC(kernel='sigmoid', gamma=0.7, C=C, tol=t, max_iter=maxiter).fit(OXlib, Oylib)\n",
    "\n",
    "pred_svcLK = svc_Lin_kernel.predict(OXlibtst)\n",
    "pred_rbf_svc = rbf_svc.predict(OXlibtst)\n",
    "pred_poly_svc = poly_svc.predict(OXlibtst)\n",
    "pred_lin_svc = lin_svc.predict(OXlibtst)\n",
    "pred_sig_svc = sig_svc.predict(OXlibtst)\n",
    "\n",
    "acc_svcLK = accuracy_score(Oylibtst, pred_svcLK)\n",
    "acc_lin_svc = accuracy_score(Oylibtst, pred_lin_svc)\n",
    "acc_rbf_svc = accuracy_score(Oylibtst, pred_rbf_svc )\n",
    "acc_poly_svc = accuracy_score(Oylibtst, pred_poly_svc)\n",
    "acc_sig_svc = accuracy_score(Oylibtst, pred_sig_svc)\n",
    "\n",
    "\n",
    "print(\"RBF accuracy\", acc_rbf_svc)\n",
    "print(\"Poly accuracy\",acc_poly_svc)\n",
    "print(\"Sigmoid accuracy\",acc_sig_svc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:snakes]",
   "language": "python",
   "name": "conda-env-snakes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
